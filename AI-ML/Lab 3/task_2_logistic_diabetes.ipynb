{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task_2_logistic_diabetes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BHdMfFcnf_Ki"
      },
      "source": [
        "## Logistic Regression Modeling for Early Stage Diabetes Risk Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hjD82xIf5y0",
        "colab_type": "text"
      },
      "source": [
        "## Part 2.1: Getting familiar with linear algebraic functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-QzPbUpf5y1",
        "colab_type": "text"
      },
      "source": [
        "#### Tasks\n",
        "- Create matrix of size 10*10 with random integer numbers\n",
        "- Compute the following linear algebric operations on the matrix using built in functions supported in Numpy, Scipy etc.\n",
        "  - Find inverse of the matrix and print it\n",
        "  - Calculate dot product of the matrix with same matrix in transpose A.AT\n",
        "  - Decompose the original matrix using eigen decomposition print the eigen values and eigen vectors\n",
        "  - Calculate jacobian matrix \n",
        "  - Calculate hessian matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSxTKyu1f5y2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d70635e7-28f8-4913-dfdd-e1f134d8d535"
      },
      "source": [
        "mat = np.random.randint(100, size = (10,10))\n",
        "type(mat)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETeQ0C96f5y5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "fffe8dd6-84c0-49f5-f319-c68639eaecf3"
      },
      "source": [
        "y = np.linalg.inv(mat)\n",
        "y "
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-8.83883034e-03, -2.10199414e-03,  1.54467930e-02,\n",
              "         1.32436026e-02,  1.32424164e-02,  3.23134342e-03,\n",
              "        -1.77107335e-02, -7.95093539e-03, -1.37899268e-02,\n",
              "        -1.33932147e-03],\n",
              "       [ 2.12803985e-02,  2.61893433e-02, -8.93126441e-03,\n",
              "        -1.63264322e-02, -3.19953121e-02, -4.60857321e-03,\n",
              "         1.11211723e-02,  2.34305836e-02,  7.61097108e-03,\n",
              "        -1.48776199e-02],\n",
              "       [ 3.16193605e-03, -6.01933694e-03, -1.76856125e-02,\n",
              "        -1.06621825e-02, -7.53481137e-03,  1.24599061e-02,\n",
              "         1.99947966e-02, -6.05836271e-03,  1.61729291e-02,\n",
              "         9.48467901e-03],\n",
              "       [-7.28979133e-03,  7.29978831e-03,  7.56435562e-03,\n",
              "         1.52819588e-03, -6.15144863e-03, -7.48228083e-03,\n",
              "        -5.19115060e-03, -3.78781692e-03,  4.07648792e-03,\n",
              "         3.73353643e-03],\n",
              "       [ 2.88024467e-03,  2.24564074e-02, -6.90902038e-03,\n",
              "         1.00853943e-02, -3.35027431e-02, -7.80136902e-03,\n",
              "        -5.23585971e-03,  3.13013292e-02,  3.00097115e-03,\n",
              "        -2.36108194e-02],\n",
              "       [-7.82895450e-03, -2.07197673e-02,  1.12992938e-02,\n",
              "        -6.05018854e-03,  4.87629627e-02,  4.32070865e-03,\n",
              "        -6.89172795e-03, -2.44013606e-02, -4.94336566e-03,\n",
              "         1.15252429e-02],\n",
              "       [-1.10234135e-03,  1.05586191e-02, -1.01330688e-02,\n",
              "         4.70836726e-03, -1.98678219e-02,  1.49215591e-03,\n",
              "         1.02808857e-02,  8.50127327e-03, -9.30619799e-04,\n",
              "        -3.23957899e-03],\n",
              "       [ 1.21154150e-02, -4.82470661e-03,  2.10732260e-05,\n",
              "         1.83617599e-03, -1.19002446e-03,  5.86424854e-04,\n",
              "         5.88621218e-03, -4.73745459e-03,  1.40608443e-03,\n",
              "        -3.89368803e-03],\n",
              "       [-3.81199379e-03, -1.47403819e-02,  2.33163521e-03,\n",
              "        -6.91413725e-03,  1.62936858e-02,  3.28094694e-03,\n",
              "         5.52901001e-03,  1.50965251e-04, -8.88774392e-04,\n",
              "         7.91502482e-03],\n",
              "       [-7.71289163e-03, -1.52181773e-02,  1.68564333e-02,\n",
              "         1.67850839e-02,  2.97950459e-02, -5.92238306e-03,\n",
              "        -2.40780542e-02, -1.84512002e-02, -1.62173018e-02,\n",
              "         1.27277424e-02]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qR3KNYXf5y8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "0d413b99-6e10-487b-93b6-3dd1bffcae24"
      },
      "source": [
        "np.dot(mat,mat.T)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[15162, 19438, 12622, 24552, 16937, 12004,  9492, 16711, 18718,\n",
              "        20332],\n",
              "       [19438, 40474, 19681, 42726, 33018, 17649, 22125, 26025, 30586,\n",
              "        31871],\n",
              "       [12622, 19681, 26188, 27433, 16336, 15267, 17501, 19494, 22963,\n",
              "        20123],\n",
              "       [24552, 42726, 27433, 54295, 37851, 20292, 28149, 33125, 39878,\n",
              "        37312],\n",
              "       [16937, 33018, 16336, 37851, 29543, 13295, 19652, 23656, 27736,\n",
              "        26193],\n",
              "       [12004, 17649, 15267, 20292, 13295, 20433,  5848, 16495, 17757,\n",
              "        18561],\n",
              "       [ 9492, 22125, 17501, 28149, 19652,  5848, 22869, 15169, 17057,\n",
              "        15688],\n",
              "       [16711, 26025, 19494, 33125, 23656, 16495, 15169, 26253, 25871,\n",
              "        28237],\n",
              "       [18718, 30586, 22963, 39878, 27736, 17757, 17057, 25871, 36453,\n",
              "        27844],\n",
              "       [20332, 31871, 20123, 37312, 26193, 18561, 15688, 28237, 27844,\n",
              "        35682]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3QXSmzff5y_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "374e3df7-0396-45e7-f446-051abd5439df"
      },
      "source": [
        "import autograd\n",
        "\n",
        "def cost(x):\n",
        "  return x[0]**2 + x[1]\n",
        "jac = autograd.jacobian(cost)\n",
        "jac(mat)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 44,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  1,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
              "\n",
              "       [[  0, 116,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   1,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
              "\n",
              "       [[  0,   0, 104,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   1,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
              "\n",
              "       [[  0,   0,   0,  20,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   1,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
              "\n",
              "       [[  0,   0,   0,   0,  58,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   1,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
              "\n",
              "       [[  0,   0,   0,   0,   0,  38,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   1,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
              "\n",
              "       [[  0,   0,   0,   0,   0,   0,  28,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   1,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
              "\n",
              "       [[  0,   0,   0,   0,   0,   0,   0,  92,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   1,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
              "\n",
              "       [[  0,   0,   0,   0,   0,   0,   0,   0,  60,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   1,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
              "\n",
              "       [[  0,   0,   0,   0,   0,   0,   0,   0,   0, 128],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TKw0pZWf5zB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfNeVQ5Vf5zE",
        "colab_type": "text"
      },
      "source": [
        "## Part 2.2: Logistic Regression using newton method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1LWrifqkf_Kj"
      },
      "source": [
        "### Logistic regression\n",
        "Logistic regression uses an equation as the representation, very much like linear regression.\n",
        "\n",
        "Input values (x) are combined linearly using weights or coefficient values (referred to as W) to predict an output value (y). A key difference from linear regression is that the output value being modeled is a binary values (0 or 1) rather than a continuous value.<br>\n",
        "\n",
        "###  $\\hat{y}(w, x) = \\frac{1}{1+exp^{-(w_0 + w_1 * x_1 + ... + w_p * x_p)}}$\n",
        "\n",
        "#### Dataset\n",
        "The dataset is available at <strong>\"data/diabetes_data.csv\"</strong> in the respective challenge's repo.<br>\n",
        "<strong>Original Source:</strong> http://archive.ics.uci.edu/ml/machine-learning-databases/00529/diabetes_data_upload.csv. The dataset just got released in July 2020.<br><br>\n",
        "\n",
        "#### Features (X)\n",
        "\n",
        "1. Age                - Values ranging from 16-90\n",
        "2. Gender             - Binary value (Male/Female)\n",
        "3. Polyuria           - Binary value (Yes/No)\n",
        "4. Polydipsia         - Binary value (Yes/No)\n",
        "5. sudden weight loss - Binary value (Yes/No)\n",
        "6. weakness           - Binary value (Yes/No)\n",
        "7. Polyphagia         - Binary value (Yes/No)\n",
        "8. Genital thrush     - Binary value (Yes/No)\n",
        "9. visual blurring    - Binary value (Yes/No)\n",
        "10. Itching           - Binary value (Yes/No)\n",
        "11. Irritability      - Binary value (Yes/No)\n",
        "12. delayed healing   - Binary value (Yes/No)\n",
        "13. partial paresis   - Binary value (Yes/No)\n",
        "14. muscle stiffness  - Binary value (Yes/No)\n",
        "15. Alopecia          - Binary value (Yes/No)\n",
        "16. Obesity           - Binary value (Yes/No)\n",
        "\n",
        "#### Output/Target target (Y) \n",
        "17. class - Binary class (Positive/Negative)\n",
        "\n",
        "#### Objective\n",
        "To learn logistic regression and practice handling of both numerical and categorical features\n",
        "\n",
        "#### Tasks\n",
        "- Download, load the data and print first 5 and last 5 rows\n",
        "- Transform categorical features into numerical features. Use label encoding or any other suitable preprocessing technique\n",
        "- Since the age feature is in larger range, age column can be normalized into smaller scale (like 0 to 1) using different methods such as scaling, standardizing or any other suitable preprocessing technique (Example - sklearn.preprocessing.MinMaxScaler class)\n",
        "- Define X matrix (independent features) and y vector (target feature)\n",
        "- Split the dataset into 60% for training and rest 40% for testing (sklearn.model_selection.train_test_split function)\n",
        "- Train Logistic Regression Model on the training set (sklearn.linear_model.LogisticRegression class)\n",
        "- Use the trained model to predict on testing set\n",
        "- Print 'Accuracy' obtained on the testing dataset i.e. (sklearn.metrics.accuracy_score function)\n",
        "\n",
        "#### Further fun (will not be evaluated)\n",
        "- Plot loss curve (Loss vs number of iterations)\n",
        "- Preprocess data with different feature scaling methods (i.e. scaling, normalization, standardization, etc) and observe accuracies on both X_train and X_test\n",
        "- Training model on different train-test splits such as 60-40, 50-50, 70-30, 80-20, 90-10, 95-5 etc. and observe accuracies on both X_train and X_test\n",
        "- Shuffling of training samples with different *random seed values* in the train_test_split function. Check the model error for the testing data for each setup.\n",
        "- Print other classification metrics such as:\n",
        "    - classification report (sklearn.metrics.classification_report),\n",
        "    - confusion matrix (sklearn.metrics.confusion_matrix),\n",
        "    - precision, recall and f1 scores (sklearn.metrics.precision_recall_fscore_support)\n",
        "\n",
        "#### Helpful links\n",
        "- Scikit-learn documentation for logistic regression: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "- How Logistic Regression works: https://machinelearningmastery.com/logistic-regression-for-machine-learning/\n",
        "- Feature Scaling: https://scikit-learn.org/stable/modules/preprocessing.html\n",
        "- Training testing splitting: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "- Classification metrics in sklearn: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
        "- Use slack for doubts: https://join.slack.com/t/deepconnectai/shared_invite/zt-givlfnf6-~cn3SQ43k0BGDrG9_YOn4g"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-i4VgviHf_Kk",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ooYDzG4SnErt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4a29dee5-3f96-4ba4-ddca-0947098fbfed"
      },
      "source": [
        "# Download the dataset from the source\n",
        "!wget _URL_"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-30 16:35:47--  http://_url_/\n",
            "Resolving _url_ (_url_)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘_url_’\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XqZrgW_if_Kq",
        "colab": {}
      },
      "source": [
        "# NOTE: DO NOT CHANGE THE VARIABLE NAME(S) IN THIS CELL\n",
        "# Load the data\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/DeepConnectAI/challenge-week-3/master/data/diabetes_data.csv\")"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0WW0J7ff5zM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "e76f008e-6919-47b0-b4bb-914aa15bcbf1"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Polyuria</th>\n",
              "      <th>Polydipsia</th>\n",
              "      <th>sudden weight loss</th>\n",
              "      <th>weakness</th>\n",
              "      <th>Polyphagia</th>\n",
              "      <th>Genital thrush</th>\n",
              "      <th>visual blurring</th>\n",
              "      <th>Itching</th>\n",
              "      <th>Irritability</th>\n",
              "      <th>delayed healing</th>\n",
              "      <th>partial paresis</th>\n",
              "      <th>muscle stiffness</th>\n",
              "      <th>Alopecia</th>\n",
              "      <th>Obesity</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>40</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>58</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>41</td>\n",
              "      <td>Male</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>45</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60</td>\n",
              "      <td>Male</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Age Gender Polyuria Polydipsia  ... muscle stiffness Alopecia Obesity     class\n",
              "0   40   Male       No        Yes  ...              Yes      Yes     Yes  Positive\n",
              "1   58   Male       No         No  ...               No      Yes      No  Positive\n",
              "2   41   Male      Yes         No  ...              Yes      Yes      No  Positive\n",
              "3   45   Male       No         No  ...               No       No      No  Positive\n",
              "4   60   Male      Yes        Yes  ...              Yes      Yes     Yes  Positive\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEhD7_w1f5zP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hjCRzhp_f_Kw",
        "colab": {}
      },
      "source": [
        "# Handle categorical/binary columns"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJVSoFVkf5zX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "outputId": "c591da7c-bf9b-4c52-ee2e-e992f96104d2"
      },
      "source": [
        "label_encoder=preprocessing.LabelEncoder()\n",
        "catcols=data.select_dtypes(include=['object']).columns.tolist()\n",
        "data[catcols]=data[catcols].apply(label_encoder.fit_transform)\n",
        "data"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Polyuria</th>\n",
              "      <th>Polydipsia</th>\n",
              "      <th>sudden weight loss</th>\n",
              "      <th>weakness</th>\n",
              "      <th>Polyphagia</th>\n",
              "      <th>Genital thrush</th>\n",
              "      <th>visual blurring</th>\n",
              "      <th>Itching</th>\n",
              "      <th>Irritability</th>\n",
              "      <th>delayed healing</th>\n",
              "      <th>partial paresis</th>\n",
              "      <th>muscle stiffness</th>\n",
              "      <th>Alopecia</th>\n",
              "      <th>Obesity</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>58</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>45</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515</th>\n",
              "      <td>39</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516</th>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>517</th>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>518</th>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>519</th>\n",
              "      <td>42</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>520 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Age  Gender  Polyuria  ...  Alopecia  Obesity  class\n",
              "0     40       1         0  ...         1        1      1\n",
              "1     58       1         0  ...         1        0      1\n",
              "2     41       1         1  ...         1        0      1\n",
              "3     45       1         0  ...         0        0      1\n",
              "4     60       1         1  ...         1        1      1\n",
              "..   ...     ...       ...  ...       ...      ...    ...\n",
              "515   39       0         1  ...         0        0      1\n",
              "516   48       0         1  ...         0        0      1\n",
              "517   58       0         1  ...         0        1      1\n",
              "518   32       0         0  ...         1        0      0\n",
              "519   42       1         0  ...         0        0      0\n",
              "\n",
              "[520 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thrdyCGWf5zZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3aNK0lA1f_Kz",
        "colab": {}
      },
      "source": [
        "# Normalize the age feature"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tqCVUtIUf_K3",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "x = data[['Age']].values\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x_scaled = min_max_scaler.fit_transform(x)\n",
        "data['Age'] = x_scaled\n"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0Uc-BEzqf_K-",
        "colab": {}
      },
      "source": [
        "# Define your X and y\n",
        "X = data.iloc[:,:-1].values\n",
        "y = data.iloc[:,-1].values"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DIiMrIaajX-Q",
        "colab": {}
      },
      "source": [
        "# Split the dataset into training and testing here\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4mGH-WTf5zk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(X, weights):\n",
        "    '''Predict class for X.\n",
        "    For the given dataset, predicted vector has only values 0/1\n",
        "    Args:\n",
        "        X : Numpy array (num_samples, num_features)\n",
        "        weights : Model weights for logistic regression\n",
        "    Returns:\n",
        "        Binary predictions : (num_samples,)\n",
        "    '''\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    z = X.dot(weights)\n",
        "    logits = sigmoid(z)\n",
        "    y_pred = np.array(list(map(lambda x: 1 if x>0.5 else 0, logits)))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return y_pred"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS2gLvlLf5zm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "        '''Sigmoid function: f:R->(0,1)\n",
        "        Args:\n",
        "            z : A numpy array (num_samples,)\n",
        "        Returns:\n",
        "            A numpy array where sigmoid function applied to every element\n",
        "        '''\n",
        "        ### START CODE HERE\n",
        "        sig_z = 1/(1 + np.exp(-z))\n",
        "\n",
        "        ### END CODE HERE\n",
        "        \n",
        "        assert (z.shape==sig_z.shape), 'Error in sigmoid implementation. Check carefully'\n",
        "        return sig_z"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YimKuBoCf5zo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    '''Calculate cross entropy loss\n",
        "    Note: Cross entropy is defined for multiple classes/labels as well\n",
        "    but for this dataset we only need binary cross entropy loss\n",
        "    Args:\n",
        "        y_true : Numpy array of true values (0/1) of size (num_samples,)\n",
        "        y_pred : Numpy array of predicted values (probabilites) of size (num_samples,)\n",
        "    Returns:\n",
        "\n",
        "        Cross entropy loss: A scalar value\n",
        "    '''\n",
        "    # Fix 0 values in y_pred\n",
        "    eps=1e-15\n",
        "    y_pred = np.maximum(np.full(y_pred.shape, eps), np.minimum(np.full(y_pred.shape, 1-eps), y_pred))\n",
        "    \n",
        "    ### START CODE HERE\n",
        "    ce_loss = np.mean(-y_true*np.log(y_pred)-(1-y_true)*np.log(1-y_pred))\n",
        "    ### END CODE HERE\n",
        "    \n",
        "    return ce_loss"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSIj6kVWf5zq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def newton_optimization(X, y, max_iterations=25):\n",
        "    '''Implement netwon method for optimizing weights\n",
        "    Args:\n",
        "        X : Numpy array (num_samples, num_features)\n",
        "        max_iterations : Max iterations to update the weights\n",
        "    Returns:\n",
        "        Optimal weights (num_features,)\n",
        "    '''\n",
        "    num_samples = X.shape[0]\n",
        "    num_features = X.shape[1]\n",
        "    # Initialize random weights\n",
        "    weights = np.zeros(num_features,)\n",
        "    # Initialize losses\n",
        "    losses = []\n",
        "    \n",
        "    # Newton Method\n",
        "    for i in range(max_iterations):\n",
        "        # Predict/Calculate probabilties using sigmoid function\n",
        "        y_p = sigmoid(X@weights)\n",
        "        \n",
        "        # Define gradient for J (cost function) i.e. cross entropy loss\n",
        "        gradient = ((1/num_samples)*X.T) @ (y_p - y)\n",
        "        \n",
        "        # Define hessian matrix for cross entropy loss\n",
        "        hessian= ((1/num_samples)*X.T) @ (np.diag(y_p)) @ (np.diag(1-y_p)) @ X\n",
        "        \n",
        "        # Update the model using hessian matrix and gradient computed\n",
        "        weights-= np.dot(np.linalg.pinv(hessian),gradient)\n",
        "        \n",
        "        # Calculate cross entropy loss\n",
        "        loss = cross_entropy_loss(y, y_p)\n",
        "        # Append it\n",
        "        losses.append(loss)\n",
        "\n",
        "    return weights, losses"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f_z1xPSf5zs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train weights\n",
        "weights, losses = newton_optimization(X_train, y_train)"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0g7EZ6Mf5zu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "8f6af60b-f233-4e85-cc01-77e927380b80"
      },
      "source": [
        "# Plot the loss curve\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot([i+1 for i in range(len(losses))], losses)\n",
        "plt.title(\"Loss curve\")\n",
        "plt.xlabel(\"Iteration num\")\n",
        "plt.ylabel(\"Cross entropy curve\")\n",
        "plt.show()"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPCu2TBBf5zw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "48f13d21-54ae-400b-d2d9-a46935064147"
      },
      "source": [
        "our_model_test_acuracy = accuracy_score(y_test, predict(X_test, weights))\n",
        "\n",
        "print(f\"\\nAccuracy in testing set by our model: {our_model_test_acuracy}\")"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy in testing set by our model: 0.9102564102564102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suRlh9NGf5zy",
        "colab_type": "text"
      },
      "source": [
        "#### Compare with the scikit learn implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qhvibx3Xf_LB",
        "colab": {}
      },
      "source": [
        "# Initialize the model\n",
        "model = LogisticRegression(solver='newton-cg', verbose=1)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ndXHgNLxf_LD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "4a4c60a5-ed20-4598-9644-5b69e923ad60"
      },
      "source": [
        "# Fit the model. Wait! We will complete this step for you ;)\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='newton-cg', tol=0.0001, verbose=1,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oHOeLfjFjeNh",
        "colab": {}
      },
      "source": [
        "# Predict on testing set X_test\n",
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eE5g0uoYf_LG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1e9a7c9c-eec9-4e97-e434-f81b5d561706"
      },
      "source": [
        "# Print Accuracy on testing set\n",
        "sklearn_test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nAccuracy in testing set by sklearn model: {sklearn_test_accuracy}\")"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy in testing set by sklearn model: 0.9102564102564102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH2gTAsvMT1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}